{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 101.0.4951\n",
      "Get LATEST chromedriver version for 101.0.4951 google-chrome\n",
      "Driver [/home/kaylanm/.wdm/drivers/chromedriver/linux64/101.0.4951.41/chromedriver] found in cache\n",
      "/tmp/ipykernel_118275/3442099113.py:34: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 101.0.4951\n",
      "Get LATEST chromedriver version for 101.0.4951 google-chrome\n",
      "Driver [/home/kaylanm/.wdm/drivers/chromedriver/linux64/101.0.4951.41/chromedriver] found in cache\n",
      "/tmp/ipykernel_118275/3442099113.py:500: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  overview_cont = self.driver.find_element_by_xpath('//*[@id=\"overview\"]')\n",
      "/home/kaylanm/miniconda3/envs/Fishwatch/lib/python3.9/site-packages/selenium/webdriver/remote/webelement.py:426: UserWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  warnings.warn(\"find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\")\n",
      "/tmp/ipykernel_118275/3442099113.py:506: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  overview_cont = self.driver.find_element_by_xpath('//*[@id=\"overview\"]')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POPULATION\\nAbove target population level.']\n",
      "['POPULATION\\nAbove target population level.', 'FISHING RATE\\nAt recommended level.']\n",
      "['POPULATION\\nAbove target population level.', 'FISHING RATE\\nAt recommended level.', 'HABITAT IMPACTS\\nArea closures and gear restrictions protect habitat that are affected by some kinds of trawl gear.']\n",
      "['POPULATION\\nAbove target population level.', 'FISHING RATE\\nAt recommended level.', 'HABITAT IMPACTS\\nArea closures and gear restrictions protect habitat that are affected by some kinds of trawl gear.', 'BYCATCH\\nRegulations are in place to minimize bycatch.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import select\n",
    "import uuid\n",
    "import random\n",
    "import json\n",
    "import urllib.request\n",
    "import boto3\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_FILES_PER_DIR = 106\n",
    "\n",
    "myuuid = uuid.uuid4().hex\n",
    "\n",
    "Individual_fishes = {}\n",
    "\n",
    "data_complete = False\n",
    "\n",
    "counter = False\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "url = []\n",
    "Name = []\n",
    "uuid_ = []\n",
    "\n",
    "Fish_data = {'Image': [], 'AKA' : [], 'Basic_info' : [], 'Facts' : [], 'Fishery' : [], 'Science' : []}\n",
    "\n",
    "Fish_data_farming = {'Image': [], 'AKA' : [], 'Basic_info' : [], 'Facts' : [], 'Farming' : [], 'Science' : []}\n",
    "\n",
    "Fish = {'Name': [] , 'URL': [], 'UUID': [], 'Image': [], 'AKA' : [], 'Basic_info' : [], 'Facts' : [], 'Fishery' : [], 'Farming': [], 'Science' : []}\n",
    "\n",
    "\n",
    "open_all_button = False\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, fish, headless=False) -> None:\n",
    "        self.fish = fish\n",
    "        self.url = f'https://www.fishwatch.gov/profiles/{fish}'\n",
    "        s = Service(ChromeDriverManager().install())\n",
    "        if headless:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument('--headless')\n",
    "            self.driver = webdriver.Remote(\"http://127.0.0.1:4444/wd/hub\", DesiredCapabilities.CHROME, options=options)\n",
    "            \n",
    "        else:\n",
    "            self.driver = webdriver.Chrome(service=s)\n",
    "        self.driver.get(self.url)\n",
    "        \n",
    "    def accept_cookies(self, xpath, iframe):\n",
    "        '''\n",
    "        This fuction allows the user to proceed through the 'Accept Cookies' screen, allowing\n",
    "        the automation to access the webpage\n",
    "        \n",
    "        Public Use: User needs to add the Xpath for the iframe on the particular webpage to use this fucntion \n",
    "        \n",
    "        '''\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            self.driver.switch_to.frame(iframe)\n",
    "            cookies_button = self.driver.find_element(By.XPATH, xpath)\n",
    "            cookies_button.click()\n",
    "        except:\n",
    "            print('There was no cookies button')\n",
    "\n",
    "    def click_search_bar(self, xpath):\n",
    "        '''\n",
    "        This function allows the automated system to click on the search bar.\n",
    "\n",
    "        User needs to provide the Xpath for the search bar\n",
    "        \n",
    "        '''\n",
    "        search_bar = self.driver.find_element(By.XPATH, xpath)\n",
    "        search_bar.click()\n",
    "        return search_bar\n",
    "\n",
    "    def typing(self, xpath: str, text: str) -> None:\n",
    "        '''\n",
    "        Function to allows the automated system to insert text onto the webpage it is scraping.\n",
    "\n",
    "        User needs to add the specified text, to use this function.\n",
    "        \n",
    "        '''\n",
    "        search_bar = self.click_search_bar(xpath)\n",
    "        search_bar.send_keys(text)\n",
    "\n",
    "    def click_wild(self, xpath):\n",
    "        wild_button = self.driver.find_element(By.XPATH, xpath)\n",
    "        wild_button.click()\n",
    "    \n",
    "    def click_farmed(self, xpath):\n",
    "        farmed_button = self.driver.find_element(By.XPATH, xpath)\n",
    "        farmed_button.click()\n",
    "        \n",
    "\n",
    "    def click_fish(self, xpath):\n",
    "        fishes = self.driver.find_element(By.XPATH, xpath) \n",
    "        fishes.click()\n",
    "       \n",
    "    def get_fish(self, class_name):\n",
    "        '''\n",
    "        This function captures the URL for each individual fish, and appends this to the 'Fish' dictionary.\n",
    "\n",
    "        If another fish has been added to the database, + 1 to 115\n",
    "        \n",
    "        User needs to add the overall class name 'seafood profile'\n",
    "        '''\n",
    "        try:\n",
    "            fishes_list = self.driver.find_elements(By.CLASS_NAME, class_name)\n",
    "            fish_url = []\n",
    "            for fish in fishes_list:\n",
    "                link = fish.get_attribute('href')\n",
    "                Fish['URL'].append(fish.get_attribute('href'))\n",
    "                if len(fish_url) == 107:\n",
    "                    pass\n",
    "    \n",
    "                else:\n",
    "                    continue \n",
    "                \n",
    "        except:\n",
    "            print('Could not obtain Fish URLs')\n",
    "            \n",
    "        \n",
    "        \n",
    "    def get_id(self, class_name):\n",
    "        '''\n",
    "        This function captures the last part of each url, using this as a name for each fish.\n",
    "\n",
    "        User needs to add the class the fish are under when introducing this fucntion, 'seafood-profile'.\n",
    "\n",
    "        Also add +1 to 115 if another fish has been added to the database.\n",
    "        \n",
    "        '''\n",
    "        try:\n",
    "            new_data = self.driver.find_elements(By.CLASS_NAME, class_name)\n",
    "            profile_name = []\n",
    "            for fish in new_data:\n",
    "                fish.get_attribute('href').rsplit('/')[-1]\n",
    "                data = fish.get_attribute('href').rsplit('/')[-1]\n",
    "                # profile_name = data.text\n",
    "                # Fish['Name'].append(data)\n",
    "                Fish['Name'].append(fish.get_attribute('href').rsplit('/')[-1])\n",
    "                if len(profile_name) == 107:\n",
    "                    pass\n",
    "                else:\n",
    "                    continue\n",
    "                [str(v) for v in Fish['Name']]\n",
    "        \n",
    "\n",
    "        except:\n",
    "            print('Could not obtain name for Fish')\n",
    "\n",
    "    def get_uuid(self, length):\n",
    "        '''\n",
    "        This function generates a random UUID for each 'Fish' in the data base.\n",
    "\n",
    "        User needs to add how many Fish are in the data base to access the function, and +1 to 115\n",
    "        \n",
    "        '''\n",
    "        try:\n",
    "            uuid_list = len(Fish['Name'])\n",
    "            print(uuid_list)\n",
    "            for i in range(108):\n",
    "        \n",
    "                # gen_uuid = uuid.uuid4()\n",
    "                Fish['UUID'].append(uuid.uuid4().hex)\n",
    "                if len(Fish['UUID']) == length:\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "                Fish['UUID'].append(uuid.uuid4().hex)\n",
    "                \n",
    "        except:\n",
    "            print('Could not generate UUID for Fish')\n",
    "\n",
    "    def click_fishes(self, xpath):\n",
    "        try:\n",
    "            fishes_container = self.driver.find_element_by_xpath(xpath) # Put this in after driver and all_profiles.click()\n",
    "            fishes_list = fishes_container.find_elements_by_xpath('//section[@class=\"seafood-profiles\"]//descendant::a')\n",
    "            num_fishes = len(fishes_list)\n",
    "            time.sleep(1)\n",
    "            for i in range(num_fishes):\n",
    "                fishes_container = self.driver.find_element_by_xpath('//section[@class=\"seafood-profiles\"]')\n",
    "                fish = fishes_container.find_elements_by_xpath('./a')[i]\n",
    "                fish.click()\n",
    "                this_url = driver.current_url\n",
    "                if this_url in df.values:\n",
    "                    print('This Object has already been scraped')\n",
    "                    counter = True\n",
    "                    pass\n",
    "                else:\n",
    "                    print('This Object has not been scraped')\n",
    "                counter = False\n",
    "                if counter == True:\n",
    "                    bot.driver.back()\n",
    "                time.sleep(2)\n",
    "                bot.get_image('//figure[@data-profile-type=\"Wild\"]')\n",
    "                time.sleep(2)\n",
    "                bot.get_aka('aka')\n",
    "                time.sleep(1)\n",
    "                bot.get_basic_info('//*[@id=\"overview\"]')\n",
    "                time.sleep(2)\n",
    "                bot.get_facts('//section[@class=\"facts\"]')\n",
    "                time.sleep(2)\n",
    "                bot.get_fishery_info('//a[@class=\"expand-toggle profiles-toggle\"]') \n",
    "                time.sleep(2)\n",
    "                \n",
    "                bot.get_science_info('science')\n",
    "                time.sleep(2)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                bot.driver.back()\n",
    "                # bot.move_images('*.png')\n",
    "        except:\n",
    "            print('Could not click all fishes')\n",
    "\n",
    "    def get_image(self, xpath): \n",
    "        '''\n",
    "        This function obtains the image of the fish on the individual fishes' page.\n",
    "\n",
    "        Needs to already be on the webpage of the fish you want to scrape for this function to work.\n",
    "\n",
    "        User needs to add the Xpath of the figure the fish image is in.\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            image_cont = self.driver.find_element(By.XPATH, xpath)\n",
    "            image = image_cont.find_element_by_tag_name('img')\n",
    "            image_link = image.get_attribute('src')\n",
    "            image_data = [] \n",
    "            image_data.append(image_link)\n",
    "            profile_name  = image.get_attribute('src').rsplit('/')[-1]\n",
    "            downloaded_image = urllib.request.urlretrieve(image_link, profile_name)\n",
    "            \n",
    "            Fish['Image'].append(image_data)\n",
    "        \n",
    "        except:\n",
    "            print('Could not obtain Fish Image')\n",
    "\n",
    "          \n",
    "    def get_aka(self, class_name): \n",
    "        '''\n",
    "        This function obtains the Fish's name and appends this to 'Fish Data'.\n",
    "\n",
    "        The name fo the class is 'aka' for each fish.\n",
    "\n",
    "        Needs to already be on the webpage of the fish you want to scrape for this function to work.\n",
    "        \n",
    "        '''\n",
    "        try:\n",
    "            data = self.driver.find_elements(By.CLASS_NAME, class_name)\n",
    "            aka_list = []\n",
    "            for fish in data:\n",
    "                \n",
    "                fish.get_attribute('innerHTML')\n",
    "                aka = aka_list.append(fish.get_attribute('innerHTML'))\n",
    "                \n",
    "                if len(aka_list) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    continue\n",
    "                Fish['AKA'].append(aka_list)\n",
    "\n",
    "        except:\n",
    "            print('Could not obtain the AKA fish name')\n",
    "\n",
    "    def get_basic_info(self, xpath):\n",
    "        '''\n",
    "        This function obtains the first section of the information, on the fishes' webpage.\n",
    "\n",
    "        User needs to add the container 'overview' to access this function.\n",
    "\n",
    "        Needs to already be on the webpage of the fish you want to scrape for this function to work.\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            overview_cont = self.driver.find_element(By.XPATH, xpath)\n",
    "            overview_list = overview_cont.find_elements_by_xpath('./a')\n",
    "            data = []\n",
    "            num_data = len(overview_list)\n",
    "\n",
    "            for i in range(num_data):\n",
    "                overview_cont = self.driver.find_element_by_xpath('//*[@id=\"overview\"]')\n",
    "                new_data = overview_cont.find_elements_by_xpath('./a')[i]\n",
    "                overview = new_data.text\n",
    "                data.append(overview)\n",
    "            Fish['Basic_info'].append(data)\n",
    "\n",
    "        except:\n",
    "            print('Could not obtain \"Basic information\" of the Fish')\n",
    "\n",
    "    def get_facts(self, xpath):\n",
    "        '''\n",
    "        This function scrapes the second section fo the Fish's database 'Facts'.\n",
    "\n",
    "        User needs to add the Xpath of this section 'facts'.\n",
    "\n",
    "        Needs to already be on the webpage of the fish you want to scrape for this function to work.\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            facts_cont = self.driver.find_element(By.XPATH, xpath)\n",
    "            facts_list = facts_cont.find_elements_by_tag_name('li')\n",
    "            data = []\n",
    "            num_facts = len(facts_list)\n",
    "\n",
    "            for i in range(num_facts):\n",
    "                facts_cont = self.driver.find_element_by_xpath('//section[@class=\"facts\"]')\n",
    "                new_data = facts_cont.find_elements_by_tag_name('li')[i]\n",
    "                facts = new_data.text\n",
    "                data.append(facts)\n",
    "            Fish['Facts'].append(data)\n",
    "\n",
    "        except:\n",
    "            print('Could not obtain data from \"Facts\" ')    \n",
    "\n",
    "\n",
    "    def get_fishery_info(self, xpath):\n",
    "        '''\n",
    "        This function clicks the open all button to allow the scraper to access all the relevent information.\n",
    "\n",
    "        Then, it scrapes the third section on the Fish's page 'Fishery'.\n",
    "\n",
    "        User needs to add the Xpath for the 'Open all' button to access this function.\n",
    "\n",
    "        Needs to already be on the webpage of the fish you want to scrape for this function to work.\n",
    "        '''\n",
    "        try:\n",
    "            open_all =  self.driver.find_element(By.XPATH, xpath)\n",
    "            open_all.click()\n",
    "            open_all_button = True\n",
    "            time.sleep(0.7)\n",
    "            \n",
    "            fishery_cont = self.driver.find_element_by_id('fishery')\n",
    "            fishery_data = fishery_cont.find_elements_by_xpath('//section[@id=\"fishery\"]//descendant::li')\n",
    "            new_data = []\n",
    "            data = []\n",
    "            fishery = []\n",
    "            num_fishery = len(fishery_data)\n",
    "\n",
    "            for i in range(num_fishery):\n",
    "                fishery_cont = self.driver.find_element_by_id('fishery')         \n",
    "                fishery_cont.find_elements_by_xpath('//section[@id=\"fishery\"]//descendant::li')[i]\n",
    "                new_data = fishery_cont.find_elements_by_xpath('//section[@id=\"fishery\"]//descendant::li')[i]\n",
    "                fishery = new_data.text\n",
    "                data.append(fishery)\n",
    "                \n",
    "            Fish['Fishery'].append(data)\n",
    "\n",
    "        except:\n",
    "            print('Could not get \"Fishery\" information')\n",
    "    \n",
    "    def get_farming_info(self, xpath):\n",
    "        '''\n",
    "        This function clicks the open all button to allow the scraper to access all the relevent information.\n",
    "\n",
    "        Then, it scrapes the third section on the Fish's page 'Farming'.\n",
    "\n",
    "        User needs to add the Xpath for the 'Open all' button to access this function.\n",
    "\n",
    "        Needs to already be on the webpage of the fish you want to scrape for this function to work.\n",
    "        '''\n",
    "        try:\n",
    "            open_all =  self.driver.find_element(By.XPATH, xpath)\n",
    "            open_all.click()\n",
    "            open_all_button = True\n",
    "            time.sleep(0.7)\n",
    "            \n",
    "            farming_cont = self.driver.find_element_by_id('farming')\n",
    "            farming_data = farming_cont.find_elements_by_xpath('//section[@id=\"farming\"]//descendant::li')\n",
    "            data = []\n",
    "            num_farming = len(farming_data)\n",
    "            for i in range(num_farming):\n",
    "                farming_cont = self.driver.find_element_by_id('farming')         \n",
    "                new_data = farming_cont.find_elements_by_xpath('//section[@id=\"farming\"]//descendant::li')[i]\n",
    "                farming = new_data.text\n",
    "                data.append(farming)\n",
    "            Fish['Farming'].append(data)\n",
    "\n",
    "        except:\n",
    "            print('Could not get \"Farming\" information')\n",
    "\n",
    "\n",
    "    def get_science_info(self, id):\n",
    "        '''\n",
    "        This function scrapes the fourth section on the Fish's page 'Science'.\n",
    "\n",
    "        User needs to add the ID for the container for the Science section 'science'.\n",
    "\n",
    "        Needs to already be on the webpage of the fish you want to scrape for this function to work.\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            science_cont = self.driver.find_element(By.ID, id)\n",
    "            science_data = science_cont.find_elements_by_xpath('//section[@id=\"science\"]//descendant::li')\n",
    "            data = []\n",
    "            num_science = len(science_data)\n",
    "            for i in range(num_science):\n",
    "                science_cont = self.driver.find_element_by_id('science')\n",
    "                new_data = science_cont.find_elements_by_xpath('//section[@id=\"science\"]//descendant::li')[i]\n",
    "                science = new_data.text\n",
    "                data.append(science)\n",
    "            Fish['Science'].append(data)\n",
    "\n",
    "        except:\n",
    "            print('Could not obtain data for the \"Science\" section')        \n",
    "\n",
    "    def scroll(self, height):\n",
    "        height = self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight\")\n",
    "        time.sleep(2)\n",
    "        self.driver.execute_script('window.scrollTo(0,' + str(height) + ');')\n",
    "    \n",
    "    def move_images(self, png):\n",
    "        '''\n",
    "        This function adds all the 'png' images to an 'Images__0' folder.\n",
    "\n",
    "        User needs to type in '*.png' to activate this function.\n",
    "        '''\n",
    "        pngDirectory = Path()\n",
    "        pngFiles = pngDirectory.glob(png)\n",
    "        for pngFile in pngFiles:\n",
    "            fileNumber = 1\n",
    "            folderNumber = int(fileNumber / MAX_FILES_PER_DIR)\n",
    "            currentFolder = pngDirectory / \"Images__{}\".format(folderNumber)\n",
    "            if not currentFolder.exists():\n",
    "                currentFolder.mkdir()\n",
    "\n",
    "            pngFile.rename(currentFolder / pngFile.name)\n",
    "    \n",
    "    def check_data(self, url, uuid__):\n",
    "\n",
    "        try:\n",
    "            chosen_url = url\n",
    "            chosen_uuid = uuid__\n",
    "            \n",
    "            if chosen_url in df.values:\n",
    "                print('Value already in Dataframe')\n",
    "                bot.driver.back()\n",
    "            elif chosen_uuid in df.values:\n",
    "                print('Value already in Dataframe')\n",
    "                bot.driver.back()\n",
    "            else:\n",
    "                print('Value not in Dataframe')\n",
    "                \n",
    "        \n",
    "        except:\n",
    "            print('Could not complete data check')\n",
    "    \n",
    "    def get_images_(self, xpath):\n",
    "        image_cont = self.driver.find_element(By.XPATH, xpath)\n",
    "        images_data = image_cont.find_elements_by_tag_name('img')\n",
    "        image_lst = len(images_data)\n",
    "        self.images_link = []\n",
    "        for i in range(image_lst):\n",
    "            image_cont = self.driver.find_element_by_xpath('//section[@id=\"page\"]')\n",
    "            image_data = image_cont.find_elements_by_xpath('//section[@id=\"page\"]//img')[i]\n",
    "            self.images_link.append(image_data.get_attribute('src'))\n",
    "            # Fish['Image'].append(self.images_link)\n",
    "            print(self.images_link)\n",
    "        self.images_link.pop()\n",
    "        return self.images_link\n",
    "    \n",
    "\n",
    "    def downloaded_image(self, path='.'):\n",
    "        if not os.path.exists(f'{path}/{self.fish}'):\n",
    "            os.makedirs(f'{path}/{self.fish}')\n",
    "        \n",
    "        if self.images_link is None:\n",
    "            print('No images found, please run get_images_() first')\n",
    "            return None\n",
    "\n",
    "        for i, scr in enumerate(tqdm(self.images_link)):\n",
    "            print(scr)\n",
    "            urllib.request.urlretrieve(scr, f'{path}/{self.fish}/{self.fish}_{i}.png')        \n",
    "    \n",
    "    def get_basic_info_for_scraper(self):\n",
    "        '''\n",
    "        This function obtains the first section of the information, on the fishes' webpage.\n",
    "\n",
    "        Needs to already be on the webpage of the fish you want to scrape for this function to work.\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            overview_cont = self.driver.find_element_by_xpath('//*[@id=\"overview\"]')\n",
    "            overview_list = overview_cont.find_elements_by_xpath('//*[@id=\"overview\"]//a')\n",
    "            data = []\n",
    "            num_data = len(overview_list)\n",
    "\n",
    "            for i in range(num_data):\n",
    "                overview_cont = self.driver.find_element_by_xpath('//*[@id=\"overview\"]')\n",
    "                new_data = overview_cont.find_elements_by_xpath('//*[@id=\"overview\"]//a')[i]\n",
    "                overview = new_data.text\n",
    "                data.append(overview)\n",
    "            print(data)\n",
    "           \n",
    "            return data\n",
    "\n",
    "        except:\n",
    "            print('Could not obtain \"Basic information\" of the Fish')    \n",
    "            \n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    bot = Scraper(\"acadian-redfish\")\n",
    "    time.sleep(0.5)\n",
    "    # bot.click_wild('/html/body/header/div/nav/ul/li[1]/ul/li/a[1]')\n",
    "    # time.sleep(2)\n",
    "    bot.get_images_('//section[@id=\"page\"]')\n",
    "\n",
    "    bot.get_fish('seafood-profile')\n",
    "    time.sleep(2)\n",
    "    bot.get_id('seafood-profile')\n",
    "    time.sleep(2)\n",
    "    bot.downloaded_image()\n",
    "    \n",
    "    bot.get_basic_info_for_scraper()\n",
    "    bot.click_fishes('//section[@class=\"seafood-profiles\"]')\n",
    "    bot.get_uuid(107)\n",
    "  \n",
    "\n",
    "    time.sleep(2)\n",
    "    driver.quit()\n",
    "\n",
    "arrays = Fish['Name'], Fish['URL'], Fish['UUID'], Fish['Image'], Fish['AKA'], Fish['Basic_info'], Fish['Facts'], Fish['Fishery'], Fish['Farming'], Fish['Science']\n",
    "max_length = 0\n",
    "for array in arrays:\n",
    "        max_length = max(max_length, len(array))\n",
    "\n",
    "for array in arrays:\n",
    "        array += ['NA'] * (max_length - len(array))\n",
    "\n",
    "df = pd.DataFrame(Fish)\n",
    "time.sleep(1)\n",
    "print(df)\n",
    "\n",
    "bot.check_data('https://www.fishwatch.gov/profiles/atlantic-salmon-farmed')\n",
    "for i in df.index:\n",
    "    time.sleep(1)\n",
    "    df.loc[i].to_json(f\"{Fish['Name'][i]}.json\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Testing Methods'\n",
    "\n",
    "import pandas as pd\n",
    "animals = {}\n",
    "df = pd.DataFrame(animals)\n",
    "print(df)\n",
    "\n",
    "\n",
    "animals = {'name': ['dog', 'monkfish', 'spider', 'frog'], 'species': ['mammal', 'fish', 'arachnid', 'anphibian'], 'height': ['0.5m', '0.3m', '0.01m', '0.1m'], 'weight': ['60kg', '30kg', '0.01kg', '0.1kg']}\n",
    "\n",
    "arrays = animals['name'], animals['species'], animals['height'], animals['weight']\n",
    "max_length = 0\n",
    "for array in arrays:\n",
    "        max_length = max(max_length, len(array))\n",
    "\n",
    "for array in arrays:\n",
    "        array += ['ayo'] * (max_length - len(array))\n",
    "# if len(animals['name']) != len(animals['species']) or len(animals['height']) or len(animals['weight']):\n",
    "#         animals['name'].append('')\n",
    "#         if len\n",
    "        \n",
    "df = pd.DataFrame(animals)\n",
    "\n",
    "for i in df.index:\n",
    "        df.loc[i].to_json(f\"{animals['name'][i]}.json\".format(i))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def __init__(self, fish, headless=False) -> None:\n",
    "#         self.fish = fish\n",
    "#         self.url = f'https://www.fishwatch.gov/profiles/{fish}'\n",
    "#         s = Chrome(ChromeDriverManager().install())\n",
    "#         if headless:\n",
    "#             options = webdriver.ChromeOptions()\n",
    "#             options.add_argument('--headless')\n",
    "#             self.driver = webdriver.Remote(\"http://127.0.0.1:4444/wd/hub\", DesiredCapabilities.CHROME, options=options)\n",
    "            \n",
    "#         else:\n",
    "#             self.driver = webdriver.Chrome(service=s)\n",
    "#         self.driver.get(self.url)\n",
    "\n",
    "'''\n",
    "Github access token docker:\n",
    "\n",
    "To use the access token from your Docker CLI client:\n",
    "\n",
    "1. Run docker login -u kaylanmistry\n",
    "\n",
    "2. At the password prompt, enter the personal access token.\n",
    "\n",
    "\n",
    "ae64ac20-17ab-4f28-a837-89c9df29b439\n",
    "\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fbf9aced17bc29cf0f72879815738b7bf76642a60064da544fd5320741c28c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('Fishwatch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
